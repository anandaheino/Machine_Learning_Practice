{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It's a classification technique based on the linear combination between X variables.\n",
    "- Also called _logit_.\n",
    "\n",
    "ex: $f(X) = \\alpha + b_1x_1 + b_2x_2 +\\dots + b_nx_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function\n",
    "- Sigmoid is a special function used in logistic regression\n",
    "- Logistic regression's goal is to find $p($**x**$)$ which the **y** = $p(x_i)$ stay closest to the real values $y_i$ for each sample $i = 1, 2, \\dots, n$\n",
    "- Here, we are going to deal with binary classification (at the first moment)\n",
    "- The outputs should be only $0$ or $1$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear combination **x** Sigmoid function\n",
    "\n",
    "- Logistic regression determines the best values for bias $\\alpha$ and weights $b_1, b_2, \\dots, b_n$\n",
    "- The function $p($**x**$)$, then, should ne closer to the real value\n",
    "- _fitting_ is the optimization process to find the best hyerparameters\n",
    "\n",
    "In this case, the input **x** to the Sigmoid function ($p$) is the output from the linear combination $f(X) = \\alpha + b_1x_1 + b_2x_2 +\\dots + b_nx_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to tune the bias and the weights?\n",
    "\n",
    "- Max (_log-likelihood_ ) is used to find the best weights\n",
    "    - This method is called _maximum likelihood estimation_ - MLE which is represented by:\n",
    "\n",
    "\\begin{equation}\n",
    "MLE = \\sum_{i=1}^n(y_i log(p(x_i)) + (1 − y_i) log(1 − p(x_i))).\n",
    "\\end{equation}\n",
    "\n",
    "- `Scikit-learn` has great mathemathical ways to maximize the hyperparameters showed above:\n",
    "   - solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, \n",
    "   - default='lbfgs'\n",
    "\n",
    "   - Algorithm to use in the optimization problem.\n",
    "\n",
    "       - For small datasets, 'liblinear' is a good choice, whereas 'sag' and 'saga' are faster for large ones.\n",
    "      - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs' handle multinomial loss; 'liblinear' is limited to one-versus-rest schemes.\n",
    "      - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n",
    "      - 'liblinear' and 'saga' also handle L1 penalty\n",
    "      - 'saga' also supports 'elasticnet' penalty\n",
    "      - 'liblinear' does not support setting ``penalty='none'``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation:\n",
    "# 1) Binary classification with one input variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating random input values\n",
    "x = np.arange(10) \n",
    "print(x)\n",
    "x.shape      #Note the shape here, we need to reshape to use this array in the LogisticRegression instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [9]]\n",
      "y = [0 0 0 0 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "x = x.reshape(-1, 1)       #after this reshaping we won't get any error message\n",
    "\n",
    "# Generating the classes [0, 1] for each sample\n",
    "y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "print(f'x = {x}')\n",
    "print(f'y = {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes = [0 1]\n",
      "bias = [-1.04608067]\n",
      "weights = [[0.51491375]]\n"
     ]
    }
   ],
   "source": [
    "# Instancing the model LogisticRegression\n",
    "\n",
    "# solver: used to optimize the parameters\n",
    "#    - liblinear: best option for SMALL and BINARY datasets.\n",
    "#    - random_state = x, no matter what number. Just to make sure it will always be the same result\n",
    "model = LogisticRegression(solver='liblinear', random_state=0)\n",
    "\n",
    "# Training the model with our data\n",
    "model.fit(x, y)\n",
    "\n",
    "# Extracting information from the model\n",
    "print(f'classes = {model.classes_}')\n",
    "print(f'bias = {model.intercept_}')\n",
    "print(f'weights = {model.coef_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities = \n",
      "[[0.74002157 0.25997843]\n",
      " [0.62975524 0.37024476]\n",
      " [0.5040632  0.4959368 ]\n",
      " [0.37785549 0.62214451]\n",
      " [0.26628093 0.73371907]\n",
      " [0.17821501 0.82178499]\n",
      " [0.11472079 0.88527921]\n",
      " [0.07186982 0.92813018]\n",
      " [0.04422513 0.95577487]\n",
      " [0.02690569 0.97309431]]\n",
      "\n",
      "estimated_class = \n",
      " [0 0 0 1 1 1 1 1 1 1]\n",
      "\n",
      "real_class = \n",
      " [0 0 0 0 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "\n",
    "# Let's look at the probabilities of each class\n",
    "probabilities = model.predict_proba(x)\n",
    "estimated_class = model.predict(x)\n",
    "\n",
    "print(f'probabilities = \\n{probabilities}\\n')\n",
    "print(f'estimated_class = \\n {estimated_class}\\n')\n",
    "print(f'real_class = \\n {y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If we compare the probabilities with the predicted values..\n",
    "1) [0.74002157 - 0.259978 ] = [class 0 - class 1]   --->  class 0 has higher probability of beeing the right class\n",
    "3) [0.5040632 - 0.4959368 ]  = [class 0 - class 1]   --->  the values are really close.\n",
    "\n",
    "- so, we have a probabilty associated with the choice of the class, and depending on the matter of the problem, we may need that value to make confident decisions.\n",
    "\n",
    "#### Example: \n",
    "   - if we are predicting if the person has cancer, based on her exams, the doctor should say that the pacient is really with cancer if the chance is almost 50/50%? Well, maybe he will think about doing the exam again checking other indicators before saying that to the pacient.\n",
    "   - on the other hand, if the probability is 95% of the pacient having cancer, the doctor should be alert about it and make an investigation into the pacient's medical record to confirm it.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Having 10 samples, the model correctly predicted 9, so the accuracy is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy is another important metric to evaluate the model\n",
    "# - values are between 0 and 1, it's based on the ( correct answers / total )  \n",
    "\n",
    "print('Having 10 samples, the model correctly predicted 9, so the accuracy is:')\n",
    "model.score(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another way of accessing the accuracy metric:\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y, estimated_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "- With the confusion matrix we can retrieve more detailed information from the classification model\n",
    "- We look at the correct answers, using 4 new metrics:\n",
    "\n",
    "    - (True negatives - **TN**): (zeros) correctly estimated\n",
    "    - (True positives - **TP**): (1's) correctly estimated\n",
    "    - (False negatives - **FN**): (zeros) wrongly estimated\n",
    "    - (False positives - **FP**): (1's) wrongly estimated\n",
    "![confusion matrix](https://miro.medium.com/max/2102/1*fxiTNIgOyvAombPJx5KGeA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CM = \n",
      "[[3 1]\n",
      " [0 6]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y, estimated_class)\n",
    "print('CM = ')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
