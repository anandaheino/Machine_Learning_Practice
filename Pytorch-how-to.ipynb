{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd7d850-3709-446b-8059-e8f76ecc0e55",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch\n",
    "\n",
    "- [PyTorch](http://pytorch.org/) it's a framework for developing and training neural networks. \n",
    "- It's very similar to numpy, but here, `array` is called `tensors`\n",
    "- `tensors` make the communication between CPU and GPU much easier than `arrays`\n",
    "- also, pytorch has usefull functions to calculate gradients (which is great for backpropagation tasks) and build neural networks\n",
    "- compared with tensorflow and other frameworks, pytorch is better to work with python / numpy / scipy\n",
    "\n",
    "Simple machine learning models (e.g. like perceptron, and linear and logistic regression) must solve linear equations like:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "representing with vectors:\n",
    "\n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "## Tensors\n",
    "\n",
    "- Linear algebra with `tensors`, matrix genaralization, and other math areas are  exactly what machine algorithms do..\n",
    "   - vector is a 1D tensor\n",
    "   - matrix is a 2D tensor\n",
    "   - a 3D array is a 3D tensor (e.g. RBG images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "964a7260-5f76-4c4f-b955-3264c1508d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46982451-b1c8-4846-9e11-305dea1d4404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_function_sigmoid(x):\n",
    "    \"\"\" Defining the activation function - Sigmoid\n",
    "    \n",
    "        Args:\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "        return: Sigmoid f(x)\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4081220b-02ab-419a-9601-288203539976",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GENERATING RANDOM DATA\n",
    "torch.manual_seed(7) # Setting the seed for replicable results\n",
    "\n",
    "# creating a tensor with 1 line (because we have only 1 sample) and 5 columns (5 features per sample), \n",
    "features = torch.randn((1, 5))     #   torch.randn ---> normal distribution with mean=0 and variance=1\n",
    "\n",
    "# generating wandom weights for the model: randn_like \n",
    "weights = torch.randn_like(features)   # it generates other tensors with the same characteristics of \"features\"\n",
    "\n",
    "# BIAS term - it's a tensor with only 1 line and 1 column\n",
    "bias = torch.randn((1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b61b3a6c-4138-4dc5-8eeb-0950fd3e1ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: tensor([[-0.1468,  0.7861,  0.9468, -1.1143,  1.6908]])\n",
      "weights:  tensor([[-0.8948, -0.3556,  1.2324,  0.1382, -1.6822]])\n",
      "bias:     tensor([[0.3177]])\n"
     ]
    }
   ],
   "source": [
    "print('features:', features)\n",
    "print('weights: ', weights)\n",
    "print('bias:    ', bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92abe124-0dc5-41a8-8c43-b70a537c0091",
   "metadata": {},
   "source": [
    "* Just like `arrays`, `tensors` can be added, subtracted, multiplied, etc.\n",
    "* the advantage here, it that we can use the GPU \n",
    "\n",
    "## Example: computing the output of a neuron model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "676a2ff8-ed47-43aa-bc83-92caa52d86e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "option 1:  tensor([[0.1595]])\n",
      "option 2:  tensor([[0.1595]])\n",
      "option 3:  tensor([[0.1595]])\n"
     ]
    }
   ],
   "source": [
    "### HOW TO PREDICT THE OUTPUT OF THE NEURON:\n",
    "\n",
    "# Just like numpy, we can use torch.sum(), assim como o m√©todos .sum() nos tensores.\n",
    "\n",
    "# torch.sum(w * f + b) which is the 1st degree equation\n",
    "y1 = activation_function_sigmoid(torch.sum(features * weights) + bias)\n",
    "print('option 1: ', y1)\n",
    "\n",
    "# .sum()\n",
    "y2 = activation_function_sigmoid((features * weights).sum() + bias)\n",
    "print('option 2: ', y2)\n",
    "\n",
    "# or we can multiply the matrixes (+effective, especially with GPUs) using torch.mm() or torch.matmul()\n",
    "#  torch.mm()\n",
    "y3 = activation_function_sigmoid(torch.mm(features, weights.view(5,1)) + bias)\n",
    "print('option 3: ', y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ad94a0-0d99-48aa-8dab-6d130c178a19",
   "metadata": {},
   "source": [
    "- As we can see, we have here 3 ways of getting to the same result!\n",
    "- Note that in 3rd option, we had to reshape our tensor weights by calling 'view(5,1)'\n",
    "- The error would be `RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x5 and 1x5)`\n",
    "- To reshape our vector, we can use:\n",
    "\n",
    "    1) `tensor.shape`\n",
    "\n",
    "    2) `tensor.reshape()`\n",
    "\n",
    "    3) `tensor.resize_()`\n",
    "\n",
    "    4) `tensor.view()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e0ebeb0-95ff-44e0-a758-5d63b9b73351",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x5 and 1x5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d14ea8ff5ba6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# ERROR: just to show how it  would be if we didnt reshape the tensor weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x5 and 1x5)"
     ]
    }
   ],
   "source": [
    "torch.mm(features, weights)   # ERROR: just to show how it  would be if we didnt reshape the tensor weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7b81f-c1b4-4323-9ff0-3e02918b93fd",
   "metadata": {},
   "source": [
    "## Converting between Numpy **and** Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "738d091c-d4de-4f7a-8190-a83fd0e3a52c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.59791751, 0.83459349, 0.28437374],\n",
       "       [0.82464193, 0.4631071 , 0.5142315 ],\n",
       "       [0.13498697, 0.85068501, 0.23941607],\n",
       "       [0.77132713, 0.76865881, 0.23185548]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# generating a random numpy array of 4 lines and 3 columns\n",
    "a = np.random.rand(4,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5ef6aca-a58d-486c-9d3c-7929feb6cbe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5979, 0.8346, 0.2844],\n",
       "        [0.8246, 0.4631, 0.5142],\n",
       "        [0.1350, 0.8507, 0.2394],\n",
       "        [0.7713, 0.7687, 0.2319]], dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting to tensor\n",
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af3b2114-e021-4255-bf6f-fdc1db87dca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.59791751, 0.83459349, 0.28437374],\n",
       "       [0.82464193, 0.4631071 , 0.5142315 ],\n",
       "       [0.13498697, 0.85068501, 0.23941607],\n",
       "       [0.77132713, 0.76865881, 0.23185548]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting back to numpy\n",
    "b.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b670dc14-6dd4-4ff3-88c6-37ea37d72dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1958, 1.6692, 0.5687],\n",
      "        [1.6493, 0.9262, 1.0285],\n",
      "        [0.2700, 1.7014, 0.4788],\n",
      "        [1.5427, 1.5373, 0.4637]], dtype=torch.float64)\n",
      "[[1.19583502 1.66918699 0.56874748]\n",
      " [1.64928385 0.9262142  1.028463  ]\n",
      " [0.26997393 1.70137001 0.47883215]\n",
      " [1.54265425 1.53731761 0.46371095]]\n"
     ]
    }
   ],
   "source": [
    "# if we change an object 'inplace', we change both objects\n",
    "# so, if we multiply 'b' by 2, in-place (passing the inplace version of mul: 'mul_')\n",
    "b.mul_(2)\n",
    "print(b)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c3df2e-e57f-477b-8679-8d5a50559686",
   "metadata": {},
   "source": [
    "## Linear Regression with PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a52be85c-2949-48e8-9f8d-2a1f33be1fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable   # variable that will be altered by gradient descent\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b72d767-6252-4bff-8d59-6860e304a742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:  1.9.0\n",
      "Using GPU:  False\n"
     ]
    }
   ],
   "source": [
    "# checking if the PyTorch version is using GPU\n",
    "print('PyTorch version: ', torch.__version__)\n",
    "print('Using GPU: ', torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    print('GPU: ',torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e3efb9-8568-4a98-8e84-5ded134f7cf1",
   "metadata": {},
   "source": [
    "- At this moment, I'm not using a GPU so the message was `False`\n",
    "### Generating random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17776d07-3b25-4e47-aa3b-400be6b96b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.],\n",
       "        [ 9.],\n",
       "        [ 3.],\n",
       "        [ 2.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = Variable(torch.Tensor([[10.0], [9.0], [3.0], [2.0]]))\n",
    "y_data = Variable(torch.Tensor([[90.0], [80.0], [50.0], [30.0]]))\n",
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a6e84c-0446-466b-8dcd-214bb013ccd2",
   "metadata": {},
   "source": [
    "### Configuring the class of our Linear Regression model\n",
    "- which will inherit from `torch.nn.Module`\n",
    "- Defining `__init__`: which inherit from the super and we add a linear layer `self.linear` with 1 feature and 1 output\n",
    "- Weights and bias are inside the layer, that why we don't define them again\n",
    "- Method `forward`: pass the instructions of the model from the input to the output\n",
    "- Linear function is very simple, so we receive $x$ and predict a value ($\\hat{y}$) for $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76dfd5f8-99f8-4dfa-b9eb-289d018ef3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)  #first is the number of features, second is number of output\n",
    "    \n",
    "    # the define our forward method, we must pass x to predict y\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "# instantiating the model    \n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346753c4-35cb-4b2b-8f38-0a060730de63",
   "metadata": {},
   "source": [
    "### Loss function (Criterion)\n",
    "\n",
    "- After executing the method `forward`, the loss function is used to compute how far if $\\hat{y}$ from $y$\n",
    "- With this loss function we can make some adjustments in the weights to get that difference closest to 0\n",
    "- We will use _Mean Square Error (MSE)_, commonly used at regression tasks.\n",
    "\n",
    "### Optimizer _Stochastic Gradient Descent (SGD)_\n",
    "- This optimizer will be used to update the weights of the model\n",
    "- the function `model.parameters()` tells our optimizer which weights will be updated\n",
    "- `lr` tells us what's going to be the learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2adf3f58-fd38-4074-8add-fd2e07f793af",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce1c7ff-cfd2-42fb-b92c-4406ce1231d8",
   "metadata": {},
   "source": [
    "### Trainning the model\n",
    "after the forward step, we modify the weights by backpropagation\n",
    "\n",
    "### Backpropagation\n",
    "1) Random weights\n",
    "2) Epochs: partial \n",
    "   - partial derivative of each weight for each layer\n",
    "   - forward step calculates the function of each neuron (a)\n",
    "   - backward\n",
    "   - each neuron loss is calculated\n",
    "   - accumulate each partial derivative\n",
    "   - Mean Gradient\n",
    "   - Use Gradient Descent to update the weights\n",
    "   - evaluate the loss function **J**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "627fb90a-cc6b-445a-adb9-acd155882b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#difining for how many epochs we will pass our model\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "       # Zero the gradients at each epoch (using backpropagation)\n",
    "    optimizer.zero_grad()\n",
    "       # Forward pass\n",
    "    y_pred = model(x_data)\n",
    "       # Computa o erro\n",
    "    loss = criterion(y_pred, y_data)\n",
    "       # propagate error to the previous layers\n",
    "    loss.backward()\n",
    "       # Atualiza os pesos\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d22f73-48c0-482b-a778-10d983d82cfb",
   "metadata": {},
   "source": [
    "### Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "626b2e86-2855-42f3-91a9-5bb6dd14f593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated value:  40.458919525146484\n"
     ]
    }
   ],
   "source": [
    "new_x = Variable(torch.Tensor([[4.0]]))\n",
    "y_pred = model(new_x)\n",
    "print(\"estimated value: \", float(y_pred.data[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00269a80-7cde-46cc-9c47-a10b1796ab05",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "- Using Fashion MNIST data \n",
    "- pixels : **28 x 28**\n",
    "- 10 classes of each type (e.g.: 10 pants, 10 shoes, 10 shirts,...)\n",
    "\n",
    "![MNIST](https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2018/05/04/ImagesSageMaker3.png)\n",
    "\n",
    "## Transforms\n",
    "- could be used for: \n",
    "   - regularization\n",
    "   - transform to tensor\n",
    "   - flip data (horizontal, vertical)\n",
    "   - image from RGB ---> b/w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0c7600f8-12d9-48f5-a263-c97ae0a6f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# transforms: we can use this to make some transformations in the original dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2525de01-cc80-4608-a76e-da1b5f636055",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Downloading datasets\n",
    "# 60.000 train samples\n",
    "# 10.000 test samples\n",
    "\n",
    "train_dataset = dsets.FashionMNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = dsets.FashionMNIST(root='./data', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7e860a-9271-4593-ad1c-ae05b567a7ee",
   "metadata": {},
   "source": [
    "### Creating data loaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f03b1ddd-f594-4fd3-aa6e-df0c69aa09f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100    # divide our 60.000 samples in 600 batches x 100 batch_size\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "input_dim = 784   # pixels 28 x 28 = 784 \n",
    "output_dim = 10   # number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfbadde-31c9-41b9-8fc8-6c2349c10a49",
   "metadata": {},
   "source": [
    "### Creating the Logistic Regression's class\n",
    "- inherit from `torch.nn.Module`\n",
    "- `__init__` method inherit from super as well\n",
    "- creating the linear parameter with input and output dimensions\n",
    "- method `forward` that receives x and return the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "30fc797a-96dc-40e6-9a89-e38b1aabe520",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6e028b-4fa6-4e69-9837-1e114f834c59",
   "metadata": {},
   "source": [
    "### Defining hiperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b3faf1b5-4714-40e9-8173-d007cc0c6da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 3000   #iterations at each batch (total 600 batches)\n",
    "# defining the epochs: in this case, 3000 / (60.000 / 100)\n",
    "epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "#input_dim = 784\n",
    "#output_dim = 10\n",
    "lr_rate = 0.001  # learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b18d58-7099-4dff-964c-b424bbcc75e2",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "- Cross-Entropy (CE), which is close to Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "### update the weights\n",
    "- using the optimizer _Stochastic Gradient Descent (SGD)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "317c292a-68f4-414a-8fa6-2a85a61bf240",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(input_dim, output_dim)\n",
    "criterion = torch.nn.CrossEntropyLoss() # computes softmax and then the cross entropy\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ccde4e-401c-4e8e-b480-ae65b9eca8f8",
   "metadata": {},
   "source": [
    "### Trainning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5247d921-c3a4-4dbb-b0e8-2c87e0c2383f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 1.5491256713867188. Accuracy: 64.05000305175781.\n",
      "Iteration: 1000. Loss: 1.4081933498382568. Accuracy: 66.0.\n",
      "Iteration: 1500. Loss: 1.2149624824523926. Accuracy: 66.63999938964844.\n",
      "Iteration: 2000. Loss: 0.9995977282524109. Accuracy: 67.9000015258789.\n",
      "Iteration: 2500. Loss: 0.9008254408836365. Accuracy: 68.80999755859375.\n",
      "Iteration: 3000. Loss: 1.092545509338379. Accuracy: 69.9000015258789.\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(int(epochs)):\n",
    "             # getting the batches (from train_loader)\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "           # creating and resizing the image from the batch\n",
    "        images = Variable(images.view(-1, 28 * 28))        # reshaping 1 vector of 784 dims\n",
    "           # creating the labels also from batch\n",
    "        labels = Variable(labels)                 # Variable: will be altered by gradient descent\n",
    "           # must zero gradients for each epoch\n",
    "        optimizer.zero_grad()\n",
    "           # forward pass to compute each output\n",
    "        outputs = model(images)\n",
    "           # loss function: cross entropy\n",
    "        loss = criterion(outputs, labels)\n",
    "           # propagating the error by backward step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iter+=1\n",
    "           # each time that iter has no remains when divided by 500, we get into this if\n",
    "           ## and print the iteration time, loss and accuracy comparing to test_loader\n",
    "        if iter%500==0:\n",
    "            # calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                outputs = model(images)\n",
    "                        # .max() gets the class with highest probability (between 10)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total+= labels.size(0)\n",
    "                # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "                correct+= (predicted == labels).sum()\n",
    "            accuracy = 100 * correct.float()/total\n",
    "            print(\"Iteration: {}. Loss: {}. Accuracy: {}.\".format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f276dec-48a6-46d5-a4e3-cc27b62f99ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 0     #count\n",
    "\n",
    "# same steps as described before\n",
    "for epoch in range(int(epochs)):\n",
    "    \n",
    "    # getting the batches (from train_loader)\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        # creating and resizing the image from the batch\n",
    "        images = Variable(images.view(-1, 28 * 28))   # reshaping 1 vector of 784 dims\n",
    "        # creating the labels also from batch\n",
    "        labels = Variable(labels)              # Variable: will be altered by gradient descent\n",
    "        \n",
    "        # must zero gradients for each epoch\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass to compute each output\n",
    "        outputs = model(images)\n",
    "        # loss function: cross entropy\n",
    "        loss = criterion(outputs, labels)\n",
    "        # propagating the error by backward step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iter+=1\n",
    "        # each time that iter has no remains when divided by 500, we get into this if\n",
    "        ## and print the iteration time, loss and accuracy comparing to test_loader\n",
    "        if iter%500==0:\n",
    "            # calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                outputs = model(images)\n",
    "                # .max() gets the class with highest probability (between 10)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total+= labels.size(0)\n",
    "                # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "                correct+= (predicted == labels).sum()\n",
    "            accuracy = 100 * correct.float()/total\n",
    "            print(\"Iteration: {}. Loss: {}. Accuracy: {}.\".format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f48598d-e102-47ec-bea9-70bc01c8c4be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef828b8-3398-4a73-b7d1-d8aff402a95d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2ae1813e-b77f-4c28-ac09-277b1b55e6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 1.667883038520813. Accuracy: 63.7400016784668.\n",
      "Iteration: 1000. Loss: 1.2943224906921387. Accuracy: 65.69000244140625.\n",
      "Iteration: 1500. Loss: 1.1537550687789917. Accuracy: 66.98999786376953.\n",
      "Iteration: 2000. Loss: 1.1066490411758423. Accuracy: 67.61000061035156.\n",
      "Iteration: 2500. Loss: 1.0060248374938965. Accuracy: 68.4000015258789.\n",
      "Iteration: 3000. Loss: 1.0104843378067017. Accuracy: 69.36000061035156.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "\n",
    "train_dataset = dsets.FashionMNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = dsets.FashionMNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "batch_size = 100\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs\n",
    "    \n",
    "n_iters = 3000\n",
    "epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "lr_rate = 0.001\n",
    "\n",
    "model = LogisticRegression(input_dim, output_dim)\n",
    "criterion = torch.nn.CrossEntropyLoss() # computes softmax and then the cross entropy\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate)\n",
    "\n",
    "iter = 0\n",
    "for epoch in range(int(epochs)):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1, 28 * 28))\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iter+=1\n",
    "        if iter%500==0:\n",
    "            # calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total+= labels.size(0)\n",
    "                # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "                correct+= (predicted == labels).sum()\n",
    "            accuracy = 100 * correct.float()/total\n",
    "            print(\"Iteration: {}. Loss: {}. Accuracy: {}.\".format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f9d091-99f8-4ebc-9aa1-ed51c5c689f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
