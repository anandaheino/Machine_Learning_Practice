{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd7d850-3709-446b-8059-e8f76ecc0e55",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch\n",
    "\n",
    "- [PyTorch](http://pytorch.org/) it's a framework for developing and training neural networks. \n",
    "- It's very similar to numpy, but here, `array` is called `tensors`\n",
    "- `tensors` make the communication between CPU and GPU much easier than `arrays`\n",
    "- also, pytorch has usefull functions to calculate gradients (which is great for backpropagation tasks) and build neural networks\n",
    "- compared with tensorflow and other frameworks, pytorch is better to work with python / numpy / scipy\n",
    "\n",
    "Simple machine learning models (e.g. like perceptron, and linear and logistic regression) must solve linear equations like:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "representing with vectors:\n",
    "\n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "## Tensors\n",
    "\n",
    "- Linear algebra with `tensors`, matrix genaralization, and other math areas are  exactly what machine algorithms do..\n",
    "   - vector is a 1D tensor\n",
    "   - matrix is a 2D tensor\n",
    "   - a 3D array is a 3D tensor (e.g. RBG images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "964a7260-5f76-4c4f-b955-3264c1508d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46982451-b1c8-4846-9e11-305dea1d4404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_function_sigmoid(x):\n",
    "    \"\"\" Defining the activation function - Sigmoid\n",
    "    \n",
    "        Args:\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "        return: Sigmoid f(x)\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4081220b-02ab-419a-9601-288203539976",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GENERATING RANDOM DATA\n",
    "torch.manual_seed(7) # Setting the seed for replicable results\n",
    "\n",
    "# creating a tensor with 1 line (because we have only 1 sample) and 5 columns (5 features per sample), \n",
    "features = torch.randn((1, 5))     #   torch.randn ---> normal distribution with mean=0 and variance=1\n",
    "\n",
    "# generating wandom weights for the model: randn_like \n",
    "weights = torch.randn_like(features)   # it generates other tensors with the same characteristics of \"features\"\n",
    "\n",
    "# BIAS term - it's a tensor with only 1 line and 1 column\n",
    "bias = torch.randn((1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b61b3a6c-4138-4dc5-8eeb-0950fd3e1ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: tensor([[-0.1468,  0.7861,  0.9468, -1.1143,  1.6908]])\n",
      "weights:  tensor([[-0.8948, -0.3556,  1.2324,  0.1382, -1.6822]])\n",
      "bias:     tensor([[0.3177]])\n"
     ]
    }
   ],
   "source": [
    "print('features:', features)\n",
    "print('weights: ', weights)\n",
    "print('bias:    ', bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92abe124-0dc5-41a8-8c43-b70a537c0091",
   "metadata": {},
   "source": [
    "* Just like `arrays`, `tensors` can be added, subtracted, multiplied, etc.\n",
    "* the advantage here, it that we can use the GPU \n",
    "\n",
    "## Example: computing the output of a neuron model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "676a2ff8-ed47-43aa-bc83-92caa52d86e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "option 1:  tensor([[0.1595]])\n",
      "option 2:  tensor([[0.1595]])\n",
      "option 3:  tensor([[0.1595]])\n"
     ]
    }
   ],
   "source": [
    "### HOW TO PREDICT THE OUTPUT OF THE NEURON:\n",
    "\n",
    "# Just like numpy, we can use torch.sum(), assim como o m√©todos .sum() nos tensores.\n",
    "\n",
    "# torch.sum(w * f + b) which is the 1st degree equation\n",
    "y1 = activation_function_sigmoid(torch.sum(features * weights) + bias)\n",
    "print('option 1: ', y1)\n",
    "\n",
    "# .sum()\n",
    "y2 = activation_function_sigmoid((features * weights).sum() + bias)\n",
    "print('option 2: ', y2)\n",
    "\n",
    "# or we can multiply the matrixes (+effective, especially with GPUs) using torch.mm() or torch.matmul()\n",
    "#  torch.mm()\n",
    "y3 = activation_function_sigmoid(torch.mm(features, weights.view(5,1)) + bias)\n",
    "print('option 3: ', y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ad94a0-0d99-48aa-8dab-6d130c178a19",
   "metadata": {},
   "source": [
    "- As we can see, we have here 3 ways of getting to the same result!\n",
    "- Note that in 3rd option, we had to reshape our tensor weights by calling 'view(5,1)'\n",
    "- The error would be `RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x5 and 1x5)`\n",
    "- To reshape our vector, we can use:\n",
    "\n",
    "    1) `tensor.shape`\n",
    "\n",
    "    2) `tensor.reshape()`\n",
    "\n",
    "    3) `tensor.resize_()`\n",
    "\n",
    "    4) `tensor.view()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e0ebeb0-95ff-44e0-a758-5d63b9b73351",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x5 and 1x5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d14ea8ff5ba6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# ERROR: just to show how it  would be if we didnt reshape the tensor weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x5 and 1x5)"
     ]
    }
   ],
   "source": [
    "torch.mm(features, weights)   # ERROR: just to show how it  would be if we didnt reshape the tensor weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7b81f-c1b4-4323-9ff0-3e02918b93fd",
   "metadata": {},
   "source": [
    "## Converting between Numpy **and** Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "738d091c-d4de-4f7a-8190-a83fd0e3a52c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.59791751, 0.83459349, 0.28437374],\n",
       "       [0.82464193, 0.4631071 , 0.5142315 ],\n",
       "       [0.13498697, 0.85068501, 0.23941607],\n",
       "       [0.77132713, 0.76865881, 0.23185548]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# generating a random numpy array of 4 lines and 3 columns\n",
    "a = np.random.rand(4,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5ef6aca-a58d-486c-9d3c-7929feb6cbe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5979, 0.8346, 0.2844],\n",
       "        [0.8246, 0.4631, 0.5142],\n",
       "        [0.1350, 0.8507, 0.2394],\n",
       "        [0.7713, 0.7687, 0.2319]], dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting to tensor\n",
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af3b2114-e021-4255-bf6f-fdc1db87dca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.59791751, 0.83459349, 0.28437374],\n",
       "       [0.82464193, 0.4631071 , 0.5142315 ],\n",
       "       [0.13498697, 0.85068501, 0.23941607],\n",
       "       [0.77132713, 0.76865881, 0.23185548]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting back to numpy\n",
    "b.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b670dc14-6dd4-4ff3-88c6-37ea37d72dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1958, 1.6692, 0.5687],\n",
      "        [1.6493, 0.9262, 1.0285],\n",
      "        [0.2700, 1.7014, 0.4788],\n",
      "        [1.5427, 1.5373, 0.4637]], dtype=torch.float64)\n",
      "[[1.19583502 1.66918699 0.56874748]\n",
      " [1.64928385 0.9262142  1.028463  ]\n",
      " [0.26997393 1.70137001 0.47883215]\n",
      " [1.54265425 1.53731761 0.46371095]]\n"
     ]
    }
   ],
   "source": [
    "# if we change an object 'inplace', we change both objects\n",
    "# so, if we multiply 'b' by 2, in-place (passing the inplace version of mul: 'mul_')\n",
    "b.mul_(2)\n",
    "print(b)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c3df2e-e57f-477b-8679-8d5a50559686",
   "metadata": {},
   "source": [
    "## Linear Regression with PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a52be85c-2949-48e8-9f8d-2a1f33be1fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b72d767-6252-4bff-8d59-6860e304a742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:  1.9.0\n",
      "Using GPU:  False\n"
     ]
    }
   ],
   "source": [
    "# checking if the PyTorch version is using GPU\n",
    "print('PyTorch version: ', torch.__version__)\n",
    "print('Using GPU: ', torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    print('GPU: ',torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e3efb9-8568-4a98-8e84-5ded134f7cf1",
   "metadata": {},
   "source": [
    "- At this moment, I'm not using a GPU so the message was `False`\n",
    "### Generating random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17776d07-3b25-4e47-aa3b-400be6b96b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.],\n",
       "        [ 9.],\n",
       "        [ 3.],\n",
       "        [ 2.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = Variable(torch.Tensor([[10.0], [9.0], [3.0], [2.0]]))\n",
    "y_data = Variable(torch.Tensor([[90.0], [80.0], [50.0], [30.0]]))\n",
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a6e84c-0446-466b-8dcd-214bb013ccd2",
   "metadata": {},
   "source": [
    "### Configuring the class of our Linear Regression model\n",
    "- which will inherit from `torch.nn.Module`\n",
    "- Defining `__init__`: which inherit from the super and we add a linear layer `self.linear` with 1 feature and 1 output\n",
    "- Weights and bias are inside the layer, that why we don't define them again\n",
    "- Method `forward`: pass the instructions of the model from the input to the output\n",
    "- Linear function is very simple, so we receive $x$ and predict a value ($\\hat{y}$) for $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76dfd5f8-99f8-4dfa-b9eb-289d018ef3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)  #first is the number of features, second is number of output\n",
    "    \n",
    "    # the define our forward method, we must pass x to predict y\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "# instantiating the model    \n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346753c4-35cb-4b2b-8f38-0a060730de63",
   "metadata": {},
   "source": [
    "## Loss function (Criterion)\n",
    "\n",
    "- After executing the method `forward`, the loss function is used to compute how far if $\\hat{y}$ from $y$\n",
    "- With this loss function we can make some adjustments in the weights to get that difference closest to 0\n",
    "- We will use _Mean Square Error (MSE)_, commonly used at regression tasks.\n",
    "\n",
    "## Optimizer _Stochastic Gradient Descent (SGD)_\n",
    "- This optimizer will be used to update the weights of the model\n",
    "- the function `model.parameters()` tells our optimizer which weights will be updated\n",
    "- `lr` tells us what's going to be the learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2adf3f58-fd38-4074-8add-fd2e07f793af",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce1c7ff-cfd2-42fb-b92c-4406ce1231d8",
   "metadata": {},
   "source": [
    "## Trainning the model\n",
    "after the forward step, we modify the weights by backpropagation\n",
    "\n",
    "## Backpropagation\n",
    "1) Random weights\n",
    "2) Epochs: partial \n",
    "   - partial derivative of each weight for each layer\n",
    "   - forward step calculates the function of each neuron (a)\n",
    "   - backward\n",
    "   - each neuron loss is calculated\n",
    "   - accumulate each partial derivative\n",
    "   - Mean Gradient\n",
    "   - Use Gradient Descent to update the weights\n",
    "   - evaluate the loss function **J**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627fb90a-cc6b-445a-adb9-acd155882b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#difining for how many epochs we will pass our model\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "       # Zero the gradients at each epoch (using backpropagation)\n",
    "    optimizer.zero_grad()\n",
    "       # Forward pass\n",
    "    y_pred = model(x_data)\n",
    "       # Computa o erro\n",
    "    loss = criterion(y_pred, y_data)\n",
    "       # propagate error to the previous layers\n",
    "    loss.backward()\n",
    "       # Atualiza os pesos\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090f5d86-c747-465a-ad6b-2986f83a0da5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
